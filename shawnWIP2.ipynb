{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to categorize the time into periods\n",
    "def categorize_time(time):\n",
    "    if time.hour < 9 or (time.hour == 9 and time.minute < 30):\n",
    "        return 'pre-market'\n",
    "    elif 9 <= time.hour < 16 or (time.hour == 9 and time.minute >= 30):\n",
    "        return 'intraday'\n",
    "    else:\n",
    "        return 'aftermarket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM\n",
      "AOS\n",
      "ABT\n",
      "ABBV\n",
      "ACN\n",
      "ADBE\n",
      "AMD\n",
      "AES\n",
      "AFL\n",
      "A\n",
      "APD\n",
      "ABNB\n",
      "AKAM\n",
      "ALB\n",
      "ARE\n",
      "ALGN\n",
      "ALLE\n",
      "LNT\n",
      "ALL\n",
      "GOOGL\n",
      "GOOG\n",
      "MO\n",
      "AMZN\n",
      "AMCR\n",
      "AEE\n",
      "AAL\n",
      "AEP\n",
      "AXP\n",
      "AIG\n",
      "AMT\n",
      "AWK\n",
      "AMP\n",
      "AME\n",
      "AMGN\n",
      "APH\n",
      "ADI\n",
      "ANSS\n",
      "AON\n",
      "APA\n",
      "AAPL\n",
      "AMAT\n",
      "APTV\n",
      "ACGL\n",
      "ADM\n",
      "ANET\n",
      "AJG\n",
      "AIZ\n",
      "T\n",
      "ATO\n",
      "ADSK\n",
      "ADP\n",
      "AZO\n",
      "AVB\n",
      "AVY\n",
      "AXON\n",
      "BKR\n",
      "BALL\n",
      "BAC\n",
      "BK\n",
      "BBWI\n",
      "BAX\n",
      "BDX\n",
      "BRK-B\n",
      "BBY\n",
      "BIO\n",
      "TECH\n",
      "BIIB\n",
      "BLK\n",
      "BX\n",
      "BA\n",
      "BKNG\n",
      "BWA\n",
      "BSX\n",
      "BMY\n",
      "AVGO\n",
      "BR\n",
      "BRO\n",
      "BF-B\n",
      "BLDR\n",
      "BG\n",
      "BXP\n",
      "CHRW\n",
      "CDNS\n",
      "CZR\n",
      "CPT\n",
      "CPB\n",
      "COF\n",
      "CAH\n",
      "KMX\n",
      "CCL\n",
      "CARR\n",
      "CTLT\n",
      "CAT\n",
      "CBOE\n",
      "CBRE\n",
      "CDW\n",
      "CE\n",
      "COR\n",
      "CNC\n",
      "CNP\n",
      "CF\n",
      "CRL\n",
      "SCHW\n",
      "CHTR\n",
      "CVX\n",
      "CMG\n",
      "CB\n",
      "CHD\n",
      "CI\n",
      "CINF\n",
      "CTAS\n",
      "CSCO\n",
      "C\n",
      "CFG\n",
      "CLX\n",
      "CME\n",
      "CMS\n",
      "KO\n",
      "CTSH\n",
      "CL\n",
      "CMCSA\n",
      "CAG\n",
      "COP\n",
      "ED\n",
      "STZ\n",
      "CEG\n",
      "COO\n",
      "CPRT\n",
      "GLW\n",
      "CPAY\n",
      "CTVA\n",
      "CSGP\n",
      "COST\n",
      "CTRA\n",
      "CRWD\n",
      "CCI\n",
      "CSX\n",
      "CMI\n",
      "CVS\n",
      "DHR\n",
      "DRI\n",
      "DVA\n",
      "DAY\n",
      "DECK\n",
      "DE\n",
      "DAL\n",
      "DVN\n",
      "DXCM\n",
      "FANG\n",
      "DLR\n",
      "DFS\n",
      "DG\n",
      "DLTR\n",
      "D\n",
      "DPZ\n",
      "DOV\n",
      "DOW\n",
      "DHI\n",
      "DTE\n",
      "DUK\n",
      "DD\n",
      "EMN\n",
      "ETN\n",
      "EBAY\n",
      "ECL\n",
      "EIX\n",
      "EW\n",
      "EA\n",
      "ELV\n",
      "EMR\n",
      "ENPH\n",
      "ETR\n",
      "EOG\n",
      "EPAM\n",
      "EQT\n",
      "EFX\n",
      "EQIX\n",
      "EQR\n",
      "ESS\n",
      "EL\n",
      "ETSY\n",
      "EG\n",
      "EVRG\n",
      "ES\n",
      "EXC\n",
      "EXPE\n",
      "EXPD\n",
      "EXR\n",
      "XOM\n",
      "FFIV\n",
      "FDS\n",
      "FICO\n",
      "FAST\n",
      "FRT\n",
      "FDX\n",
      "FIS\n",
      "FITB\n",
      "FSLR\n",
      "FE\n",
      "FI\n",
      "FMC\n",
      "F\n",
      "FTNT\n",
      "FTV\n",
      "FOXA\n",
      "FOX\n",
      "BEN\n",
      "FCX\n",
      "GRMN\n",
      "IT\n",
      "GE\n",
      "GEHC\n",
      "GEV\n",
      "GEN\n",
      "GNRC\n",
      "GD\n",
      "GIS\n",
      "GM\n",
      "GPC\n",
      "GILD\n",
      "GPN\n",
      "GL\n",
      "GDDY\n",
      "GS\n",
      "HAL\n",
      "HIG\n",
      "HAS\n",
      "HCA\n",
      "DOC\n",
      "HSIC\n",
      "HSY\n",
      "HES\n",
      "HPE\n",
      "HLT\n",
      "HOLX\n",
      "HD\n",
      "HON\n",
      "HRL\n",
      "HST\n",
      "HWM\n",
      "HPQ\n",
      "HUBB\n",
      "HUM\n",
      "HBAN\n",
      "HII\n",
      "IBM\n",
      "IEX\n",
      "IDXX\n",
      "ITW\n",
      "INCY\n",
      "IR\n",
      "PODD\n",
      "INTC\n",
      "ICE\n",
      "IFF\n",
      "IP\n",
      "IPG\n",
      "INTU\n",
      "ISRG\n",
      "IVZ\n",
      "INVH\n",
      "IQV\n",
      "IRM\n",
      "JBHT\n",
      "JBL\n",
      "JKHY\n",
      "J\n",
      "JNJ\n",
      "JCI\n",
      "JPM\n",
      "JNPR\n",
      "K\n",
      "KVUE\n",
      "KDP\n",
      "KEY\n",
      "KEYS\n",
      "KMB\n",
      "KIM\n",
      "KMI\n",
      "KKR\n",
      "KLAC\n",
      "KHC\n",
      "KR\n",
      "LHX\n",
      "LH\n",
      "LRCX\n",
      "LW\n",
      "LVS\n",
      "LDOS\n",
      "LEN\n",
      "LLY\n",
      "LIN\n",
      "LYV\n",
      "LKQ\n",
      "LMT\n",
      "L\n",
      "LOW\n",
      "LULU\n",
      "LYB\n",
      "MTB\n",
      "MRO\n",
      "MPC\n",
      "MKTX\n",
      "MAR\n",
      "MMC\n",
      "MLM\n",
      "MAS\n",
      "MA\n",
      "MTCH\n",
      "MKC\n",
      "MCD\n",
      "MCK\n",
      "MDT\n",
      "MRK\n",
      "META\n",
      "MET\n",
      "MTD\n",
      "MGM\n",
      "MCHP\n",
      "MU\n",
      "MSFT\n",
      "MAA\n",
      "MRNA\n",
      "MHK\n",
      "MOH\n",
      "TAP\n",
      "MDLZ\n",
      "MPWR\n",
      "MNST\n",
      "MCO\n",
      "MS\n",
      "MOS\n",
      "MSI\n",
      "MSCI\n",
      "NDAQ\n",
      "NTAP\n",
      "NFLX\n",
      "NEM\n",
      "NWSA\n",
      "NWS\n",
      "NEE\n",
      "NKE\n",
      "NI\n",
      "NDSN\n",
      "NSC\n",
      "NTRS\n",
      "NOC\n",
      "NCLH\n",
      "NRG\n",
      "NUE\n",
      "NVDA\n",
      "NVR\n",
      "NXPI\n",
      "ORLY\n",
      "OXY\n",
      "ODFL\n",
      "OMC\n",
      "ON\n",
      "OKE\n",
      "ORCL\n",
      "OTIS\n",
      "PCAR\n",
      "PKG\n",
      "PANW\n",
      "PARA\n",
      "PH\n",
      "PAYX\n",
      "PAYC\n",
      "PYPL\n",
      "PNR\n",
      "PEP\n",
      "PFE\n",
      "PCG\n",
      "PM\n",
      "PSX\n",
      "PNW\n",
      "PNC\n",
      "POOL\n",
      "PPG\n",
      "PPL\n",
      "PFG\n",
      "PG\n",
      "PGR\n",
      "PLD\n",
      "PRU\n",
      "PEG\n",
      "PTC\n",
      "PSA\n",
      "PHM\n",
      "QRVO\n",
      "PWR\n",
      "QCOM\n",
      "DGX\n",
      "RL\n",
      "RJF\n",
      "RTX\n",
      "O\n",
      "REG\n",
      "REGN\n",
      "RF\n",
      "RSG\n",
      "RMD\n",
      "RVTY\n",
      "ROK\n",
      "ROL\n",
      "ROP\n",
      "ROST\n",
      "RCL\n",
      "SPGI\n",
      "CRM\n",
      "SBAC\n",
      "SLB\n",
      "STX\n",
      "SRE\n",
      "NOW\n",
      "SHW\n",
      "SPG\n",
      "SWKS\n",
      "SJM\n",
      "SW\n",
      "SNA\n",
      "SOLV\n",
      "SO\n",
      "LUV\n",
      "SWK\n",
      "SBUX\n",
      "STT\n",
      "STLD\n",
      "STE\n",
      "SYK\n",
      "SMCI\n",
      "SYF\n",
      "SNPS\n",
      "SYY\n",
      "TMUS\n",
      "TROW\n",
      "TTWO\n",
      "TPR\n",
      "TRGP\n",
      "TGT\n",
      "TEL\n",
      "TDY\n",
      "TFX\n",
      "TER\n",
      "TSLA\n",
      "TXN\n",
      "TXT\n",
      "TMO\n",
      "TJX\n",
      "TSCO\n",
      "TT\n",
      "TDG\n",
      "TRV\n",
      "TRMB\n",
      "TFC\n",
      "TYL\n",
      "TSN\n",
      "USB\n",
      "UBER\n",
      "UDR\n",
      "ULTA\n",
      "UNP\n",
      "UAL\n",
      "UPS\n",
      "URI\n",
      "UNH\n",
      "UHS\n",
      "VLO\n",
      "VTR\n",
      "VLTO\n",
      "VRSN\n",
      "VRSK\n",
      "VZ\n",
      "VRTX\n",
      "VTRS\n",
      "VICI\n",
      "V\n",
      "VST\n",
      "VMC\n",
      "WRB\n",
      "GWW\n",
      "WAB\n",
      "WBA\n",
      "WMT\n",
      "DIS\n",
      "WBD\n",
      "WM\n",
      "WAT\n",
      "WEC\n",
      "WFC\n",
      "WELL\n",
      "WST\n",
      "WDC\n",
      "WY\n",
      "WMB\n",
      "WTW\n",
      "WYNN\n",
      "XEL\n",
      "XYL\n",
      "YUM\n",
      "ZBRA\n",
      "ZBH\n",
      "ZTS\n"
     ]
    }
   ],
   "source": [
    "# Pull in the list of companies\n",
    "\n",
    "df_SP_500 = pd.read_csv(\"constituents.csv\")\n",
    "ticker_list = df_SP_500['Symbol'].astype(str).tolist()\n",
    "ticker_list = [t.replace('.', '-') for t in ticker_list]\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    print(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not found for AOS\n",
      "Data not found for GOOGL\n",
      "Data not found for AJG\n",
      "Data not found for BRK-B\n",
      "Data not found for BF-B\n",
      "Data not found for CHRW\n",
      "Data not found for CE\n",
      "Data not found for CHD\n",
      "Data not found for CMCSA\n",
      "Data not found for CPAY\n",
      "Data not found for DAY\n",
      "Data not found for DHI\n",
      "Data not found for EQR\n",
      "Data not found for EL\n",
      "Data not found for FOXA\n",
      "Data not found for GEV\n",
      "Data not found for HBAN\n",
      "Data not found for JBHT\n",
      "Data not found for JPM\n",
      "Data not found for KKR\n",
      "Data not found for LOW\n",
      "Data not found for MTB\n",
      "Data not found for MMC\n",
      "Data not found for MKC\n",
      "Data not found for MRK\n",
      "Data not found for MPWR\n",
      "Data not found for NDAQ\n",
      "Data not found for NWSA\n",
      "Data not found for NWS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shawn\\AppData\\Local\\Temp\\ipykernel_4892\\4252466883.py:10: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_test['time_published'] = pd.to_datetime(df_test['time_published'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not found for CRM\n",
      "Data not found for SJM\n",
      "Data not found for SW\n",
      "Data not found for SOLV\n",
      "Data not found for TROW\n",
      "Data not found for USB\n",
      "Data not found for WRB\n",
      "Data not found for GWW\n",
      "DataFrame exported to Final_File/combined.csv\n"
     ]
    }
   ],
   "source": [
    "# create the placeholder data frame\n",
    "holding_df = pd.DataFrame()\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df_test = pd.read_csv(f'Media_Sentiment_Scores\\{ticker}_Sentiments.csv')\n",
    "\n",
    "        # Convert 'time_published' to datetime\n",
    "        df_test['time_published'] = pd.to_datetime(df_test['time_published'])\n",
    "\n",
    "        # Add a new column that creates a relevance weighted sentiment score\n",
    "        df_test['relevance_weighted_sentiment_score'] = df_test['relevance_score'] * df_test['ticker_sentiment_score']\n",
    "\n",
    "        # Apply the time periods function\n",
    "        df_test['time_period'] = df_test['time_published'].apply(categorize_time)\n",
    "\n",
    "        # Define the impacted trading day\n",
    "        df_test['date'] = df_test['time_published'] + timedelta(hours=8)\n",
    "        df_test['date'] = df_test['date'].dt.date\n",
    "\n",
    "        df_test = df_test.drop(['title','url','time_published','summary','ticker','relevance_score','ticker_sentiment_label'], axis=1)\n",
    "\n",
    "        # # Display the DataFrame\n",
    "        # display(df_test)\n",
    "\n",
    "        # Group by 'trading_date' and 'time_period' and calculate mean sentiment scores\n",
    "        grouped = df_test.groupby(['date', 'time_period']).agg({\n",
    "                    'ticker_sentiment_score': 'mean',\n",
    "                    'relevance_weighted_sentiment_score': 'mean'\n",
    "                    }).reset_index()\n",
    "\n",
    "        # Pivot the DataFrame\n",
    "        pivoted = grouped.pivot(index='date', columns='time_period', values=['ticker_sentiment_score', 'relevance_weighted_sentiment_score'])\n",
    "        pivoted.columns = [f'{col[0]}_{col[1]}' for col in pivoted.columns]\n",
    "        pivoted.reset_index(inplace=True)\n",
    "        pivoted.fillna(0, inplace=True)\n",
    "\n",
    "        # display(pivoted)\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df_test_too = pd.read_csv(f'2023_Closing_Stock_Prices\\{ticker}_stock_data.csv')\n",
    "\n",
    "        # Convert 'time_published' to datetime\n",
    "        df_test_too['date'] = pd.to_datetime(df_test_too['date'])\n",
    "        pivoted['date'] = pd.to_datetime(pivoted['date'])\n",
    "\n",
    "        # Perform a left merge\n",
    "        merged_df = df_test_too.merge(pivoted, on='date', how='left')\n",
    "        merged_df.reset_index(drop=True, inplace=True)\n",
    "        merged_df.fillna(0, inplace=True)\n",
    "\n",
    "        # display(merged_df)\n",
    "\n",
    "        sector = df_SP_500.loc[df_SP_500[\"Symbol\"] == ticker, \"GICS Sector\"].values[0]\n",
    "        merged_df['GICS Sector'] = sector\n",
    "\n",
    "        industry = df_SP_500.loc[df_SP_500[\"Symbol\"] == ticker, \"GICS Sub-Industry\"].values[0]\n",
    "        merged_df['GICS Sub-Industry'] = industry\n",
    "\n",
    "        # display(merged_df)\n",
    "        holding_df = pd.concat([holding_df, merged_df], ignore_index=True)\n",
    "\n",
    "    except:\n",
    "        print(f\"Data not found for {ticker}\")\n",
    "\n",
    "file_path = 'Final_File/combined.csv'\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "holding_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame exported to {file_path}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
